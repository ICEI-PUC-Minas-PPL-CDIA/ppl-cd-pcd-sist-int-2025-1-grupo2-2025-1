# Explica√ß√£o c√≥digo *modelo2_remasterizado.ipynb*
### C√≥digo em pyhton:
    # --- Etapa 1: Configura√ß√£o do Ambiente, Upload e Carregamento ---
    
    # Importar bibliotecas necess√°rias
    import pandas as pd
    import numpy as np
    import re
    import io
    from google.colab import files
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.preprocessing import OrdinalEncoder, StandardScaler
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    import lightgbm as lgb
    import matplotlib.pyplot as plt
    import seaborn as sns
    import warnings
    
    # Instalar/Importar bibliotecas opcionais
    try:
        from unidecode import unidecode
    except ImportError:
        print("Instalando unidecode para remover acentos...")
        !pip install unidecode
        from unidecode import unidecode
        print("Unidecode instalado.")
    
    try:
        import shap
    except ImportError:
        print("Instalando shap para interpreta√ß√£o do modelo...")
        !pip install shap
        import shap
        print("Shap instalado.")
    
    try:
        import graphviz
    except ImportError:
        print("Instalando graphviz para visualiza√ß√£o de √°rvore...")
        !apt-get install -qq graphviz > /dev/null # Instala depend√™ncia do sistema no Colab
        !pip install graphviz > /dev/null
        import graphviz
        print("Graphviz instalado.")
    
    # Ignorar warnings espec√≠ficos que podem poluir a sa√≠da
    warnings.filterwarnings("ignore", category=UserWarning, module='shap')
    warnings.filterwarnings("ignore", category=FutureWarning)
## üñ•Ô∏è Explica√ß√£o (Etapa 1 - Setup):
### Importa√ß√µes Principais: **Importa as bibliotecas essenciais:**
  - pandas e numpy: **Para manipula√ß√£o de dados (DataFrames, arrays).**
  - re: **Para express√µes regulares (usadas na limpeza).**
  - io, google.colab.files: **Para lidar com upload/leitura de arquivos no Google Colab.**
  - sklearn.model_selection: **Para dividir os dados (train_test_split) e otimizar par√¢metros (GridSearchCV).**
  - sklearn.preprocessing: **Para codificar vari√°veis ordinais (OrdinalEncoder) e escalar dados (StandardScaler).**
  - sklearn.metrics: **Para calcular m√©tricas de avalia√ß√£o (MAE, RMSE, R¬≤).**
  - lightgbm: **Para usar o algoritmo LightGBM (o modelo principal).**
  - matplotlib.pyplot, seaborn: **Para gerar gr√°ficos.**

### Warnings: 
- Para suprimir mensagens de aviso n√£o cr√≠ticas.

### Importar unidecode. 
- Se falhar (n√£o instalada), imprime uma mensagem, usa !pip install para instal√°-la no ambiente Colab e depois importa. Essencial para remover acentos na limpeza de nomes/valores.

### Fazer o mesmo para shap (interpreta√ß√£o do modelo) e graphviz (visualiza√ß√£o de √°rvore). 
- A instala√ß√£o do graphviz no Colab requer tamb√©m um comando de sistema (!apt-get).

### Ignorar Warnings: 
Configura para n√£o exibir certos tipos de avisos (UserWarning do shap, FutureWarning) que podem ser informativos mas poluem a sa√≠da principal.

---

    # --- Upload dos Arquivos ---
    print("Por favor, fa√ßa o upload do arquivo survey limpo (ex: 'survey_cleaned-1.csv'):")
    uploaded_survey = files.upload()
    # ... (c√≥digo de verifica√ß√£o e nome do arquivo) ...
    print(f"\nArquivo '{survey_filename}' carregado.")
    
    print("\n(Opcional) Por favor, fa√ßa o upload do arquivo de microdados...")
    uploaded_microdados = files.upload()
    # ... (c√≥digo de verifica√ß√£o e nome do arquivo) ...

## üì§ Explica√ß√£o (Upload):

### Solicita√ß√£o de Upload: 
- Usa files.upload() do google.colab para criar um widget interativo que permite ao usu√°rio carregar os arquivos CSV necess√°rios diretamente no ambiente Colab.

### Verifica√ß√£o: 
- O c√≥digo verifica se um arquivo foi realmente carregado antes de prosseguir.

---

    # --- Carregamento dos Dados ---
    try:
        # ... (l√≥gica complexa para tentar ler CSV com diferentes separadores e formatos de cabe√ßalho) ...
        df = pd.read_csv(...) # ou df = pd.read_csv(..., sep=';', ...)
        print(f"\nDataset Survey carregado. Shape: {df.shape}")
    except Exception as e:
        print(f"Erro CR√çTICO ao carregar Survey: {e}")
        raise
    
    # ... (c√≥digo similar para df_microdados, se carregado) ...

## ‚Üª Explica√ß√£o (Load):

### Leitura do CSV: 
- Utiliza pd.read_csv() para carregar o arquivo CSV (uploaded_survey) em um DataFrame do pandas chamado df.

### Tratamento Flex√≠vel: 
- O bloco try-except tenta lidar com diferentes cen√°rios:

- Verifica se o cabe√ßalho parece uma "tupla" (como ('P1_l ', 'Nivel de Ensino')) ou uma string normal.

- Tenta usar , como separador e, se falhar ou resultar em poucas colunas, tenta usar ;.

- Usa encoding='utf-8' para compatibilidade com caracteres diversos.

### Informa√ß√£o: 
- Imprime o formato (shape) do DataFrame carregado.

---

    # --- Fun√ß√£o de Limpeza REFINADA ---
    def clean_col_names_refined(df_to_clean):
        # ... (l√≥gica para extrair nome de tupla-like ou string) ...
        # Limpeza Refinada:
        clean_name = col_str.lower() # Lowercase primeiro
        clean_name = unidecode(clean_name) # Remove acentos
        clean_name = re.sub(r'[?()/,.:#\-\[\]\'"]+', '', clean_name) # Remove pontua√ß√£o
        clean_name = clean_name.replace(' ', '_') # Espa√ßos -> underscore
        clean_name = re.sub(r'[^a-z0-9_]+', '_', clean_name) # Remove outros caracteres especiais
        clean_name = re.sub(r'_+', '_', clean_name) # Consolida underscores
        clean_name = clean_name.strip('_') # Remove do in√≠cio/fim
        # ... (l√≥gica para garantir nome √∫nico e renomear colunas) ...
        return df_cleaned
    
    # Aplicar limpeza
    df = clean_col_names_refined(df)
    print("\nNomes das colunas do Survey (AP√ìS LIMPEZA REFINADA):")
    # print(df.columns.tolist())

## üßπ Explica√ß√£o (Clean - Nomes de Colunas):

### Objetivo: 
- Padronizar os nomes das colunas para facilitar o acesso e evitar erros.

###  Fun√ß√£o clean_col_names_refined:

- Itera sobre cada nome de coluna original.

- Tenta extrair um nome significativo caso o original seja uma string representando uma tupla (ex: ('P1_l ', 'Nivel de Ensino') -> nivel_de_ensino).

- Aplica uma s√©rie de transforma√ß√µes:

    - lower(): **Converte para min√∫sculas.**

    - unidecode(): **Remove acentos (ex: experi√™ncia -> experiencia).**

    - re.sub(): **Usa express√µes regulares para remover pontua√ß√µes e caracteres especiais, substituindo-os por _ ou removendo-os.**

    - replace(' ', '_'): **Substitui espa√ßos por underscores.**

    - **strip('_'): Remove underscores no in√≠cio ou fim.**

    - **Garante que nomes n√£o fiquem vazios e adiciona sufixos num√©ricos (_1, _2) se a limpeza gerar nomes duplicados.**

### Aplica√ß√£o: 
- A fun√ß√£o √© chamada para limpar os nomes das colunas do DataFrame df.

---

    # --- Etapa 2: Defini√ß√£o da Vari√°vel Alvo e Sele√ß√£o de Features ---
    
    target_column_clean_final = 'faixa_salarial' # Nome esperado ap√≥s limpeza
    
    feature_columns_expected_clean = [ # Lista de nomes limpos esperados
        'nivel_de_ensino', 'area_de_formacao',
        'quanto_tempo_de_experiencia_na_area_de_dados_voce_tem', 'nivel',
        'sql', 'r', 'python', # ... etc ...
    ]
    
    # Verifica√ß√£o e Sele√ß√£o
    # ... (c√≥digo para verificar se target e features existem em df.columns) ...
    valid_feature_columns_final = [col for col in feature_columns_expected_clean if col in df.columns]
    # ... (Aviso se features esperadas n√£o foram encontradas) ...
    
    df_model = df[valid_feature_columns_final + [target_column_clean_final]].copy()
    print(f"Shape inicial df_model: {df_model.shape}")

## üîé Explica√ß√£o (Select):

### Defini√ß√£o: 
- Define explicitamente o nome esperado da coluna alvo (target_column_clean_final) e uma lista com os nomes limpos esperados das colunas que ser√£o usadas como features (feature_columns_expected_clean).

### Valida√ß√£o: 
- O c√≥digo verifica se a coluna alvo e as features listadas realmente existem no DataFrame df (ap√≥s a limpeza). Isso √© importante para capturar erros de digita√ß√£o ou problemas na etapa de limpeza. Avisa se alguma feature esperada n√£o foi encontrada.

### Cria√ß√£o do df_model: 
- Cria um novo DataFrame (df_model) contendo apenas a coluna alvo e as features v√°lidas encontradas. Isso foca a an√°lise subsequente apenas nos dados relevantes. .copy() evita warnings sobre modificar o DataFrame original.

---

    # --- Etapa 3: Pr√©-processamento e Engenharia de Vari√°veis (CORRIGIDA) ---
    
    # 3.1 Tratar Sal√°rio (Vari√°vel Alvo)
    def get_salary_midpoint(salary_range):
        # ... (l√≥gica para converter faixa ex: 'de R$ 4.001 a R$ 6.000' para 5000.5) ...
        return midpoint_value # ou np.nan se n√£o conseguir converter
    
    df_model['salarymidpoint'] = df_model[target_column_clean_final].apply(get_salary_midpoint)
    df_model.dropna(subset=['salarymidpoint'], inplace=True)
    # ... (prints informativos) ...

## ‚è≥ Explica√ß√£o (Preprocess - Target):

### Objetivo: Converter a coluna alvo (faixa salarial, que √© texto) em um valor num√©rico para que o modelo de regress√£o possa prev√™-la.

### Fun√ß√£o get_salary_midpoint:
- Recebe uma string de faixa salarial.
- Limpa a string (remove 'R$', '.', ajusta separadores).
- Trata casos especiais ('menos de 1.000', 'mais de 40.001').
- Usa express√µes regulares (re.findall) para extrair os n√∫meros limites da faixa.
- Calcula o ponto m√©dio entre os limites. Considera a indica√ß√£o 'ms' (mil) se presente.
- Retorna o valor num√©rico do ponto m√©dio ou np.nan se a convers√£o falhar.

### Aplica√ß√£o: 
- A fun√ß√£o √© aplicada (.apply()) √† coluna alvo original (target_column_clean_final) para criar a nova coluna num√©rica salarymidpoint.

### Remo√ß√£o de NaN: 
- Linhas onde n√£o foi poss√≠vel calcular o salarymidpoint s√£o removidas (.dropna()) pois n√£o podem ser usadas para treinar/avaliar o modelo.

---

    # 3.2 Tratamento de Valores Nulos (ANTES DA CODIFICA√á√ÉO)
    print("\nTratando valores nulos...")
    numeric_cols_pre_encoding = []
    categorical_cols_pre_encoding = []
    
    for col in valid_feature_columns_final:
        # ... (l√≥gica para detectar flags bin√°rias vs outras colunas) ...
        numeric_col = pd.to_numeric(df_model[col], errors='coerce')
        if not numeric_col.isnull().all(): # Se conseguiu converter para n√∫mero
            if is_binary_flag:
                df_model[col] = numeric_col.fillna(0).astype(int) # Flags NaN -> 0
            else:
                # df_model[col] = numeric_col.fillna(numeric_col.median()) # Outras num√©ricas NaN -> Mediana (se houvesse)
            numeric_cols_pre_encoding.append(col)
        else: # Se era string ou falhou convers√£o num√©rica
            df_model[col] = df_model[col].astype(str).fillna('Desconhecido') # Categ√≥ricas NaN -> 'Desconhecido'
            categorical_cols_pre_encoding.append(col)
## üõ†Ô∏è Explica√ß√£o (Preprocess - Nulls):

### Objetivo: Lidar com valores ausentes (NaN) nas colunas de features, pois a maioria dos modelos n√£o os aceita.

### Estrat√©gia: Itera sobre as features v√°lidas:
- Flags Bin√°rias (P4d..., P2o...): Tenta converter para n√∫mero. Se for flag bin√°ria e tiver NaN, preenche com 0 (assumindo que NaN significa "n√£o usa" ou "n√£o considera").
- Outras Num√©ricas (se existissem): Preencheria NaN com a mediana da coluna (medida robusta a outliers).
- Categ√≥ricas (Strings): Preenche NaN com a string literal 'Desconhecido', criando uma nova categoria expl√≠cita para valores ausentes. Converte toda a coluna para string (.astype(str)) para garantir consist√™ncia antes da codifica√ß√£o.

### Listas de Rastreio: 
- Mant√©m listas separadas (numeric_cols_pre_encoding, categorical_cols_pre_encoding) para saber quais colunas tratar como num√©ricas ou categ√≥ricas na pr√≥xima etapa.

---

    # 3.3 Codifica√ß√£o de Vari√°veis Categ√≥ricas (Ordinal e Nominal - CORRIGIDA)
    # ... (defini√ß√£o dos nomes limpos das colunas ordinais: edu_col_clean, exp_col_clean, nivel_col_clean) ...
    
    # Definir colunas ordinais e suas ordens EXATAS (valores limpos)
    ordinal_cols_mapping = {
        edu_col_clean: ['nao_tenho_graduacao_formal', 'estudante_de_graduacao', ...],
        exp_col_clean: ['nao_tenho_experiencia_na_area_de_dados', 'menos_de_1_ano', ...],
        nivel_col_clean: ['junior', 'pleno', 'senior']
    }
    
    # --- Verifica√ß√£o Din√¢mica e Aplica√ß√£o do OrdinalEncoder ---
    # ... (c√≥digo que limpa valores nas colunas, compara com a ordem predefinida,
    #      cria 'final_order' com categorias v√°lidas, e popula 'ordinal_cols_to_encode',
    #      'ordinal_categories_final', 'processed_categoricals') ...
    
    # Aplicar OrdinalEncoder
    if ordinal_cols_to_encode:
        encoder_ordinal = OrdinalEncoder(
            categories=ordinal_categories_final,
            handle_unknown='use_encoded_value',
            unknown_value=-1 # Atribui -1 a valores desconhecidos/extras
        )
        df_model[ordinal_cols_to_encode] = encoder_ordinal.fit_transform(df_model[ordinal_cols_to_encode])
        # ... (atualiza lista de colunas num√©ricas) ...
    
    # Identificar colunas nominais (categ√≥ricas n√£o ordinais)
    nominal_cols_to_encode = [ ... ]
    
    # Aplicar One-Hot Encoding (get_dummies) APENAS nas nominais
    if nominal_cols_to_encode:
        df_model = pd.get_dummies(df_model, columns=nominal_cols_to_encode, dummy_na=False, drop_first=False)
        # ... (prints informativos) ...
    
    # Atualizar lista de features finais
    features_final = [ ... ]
    print(f"N√∫mero final de features: {len(features_final)}")

## üîÑ Explica√ß√£o (Preprocess - Encoding - Corre√ß√£o Principal):

### Objetivo: 
- Converter as colunas categ√≥ricas (texto) em n√∫meros para que o LightGBM possa process√°-las, respeitando a natureza de cada vari√°vel (ordinal vs. nominal).

### Vari√°veis Ordinais:
- Mapeamento: 
    - Define um dicion√°rio (ordinal_cols_mapping) que especifica quais colunas s√£o ordinais (nivel_de_ensino, quanto_tempo_de_experiencia..., nivel) e a ordem correta das suas categorias (ex: 'junior' < 'pleno' < 'senior'). Usa os nomes limpos das colunas e os valores esperados ap√≥s a limpeza (lowercase, sem acento, com underscore).
    Verifica√ß√£o Din√¢mica: 
    - O c√≥digo crucial aqui verifica, para cada coluna ordinal definida:
    - Limpa os valores reais da coluna no DataFrame.
        - Compara com a ordem predefinida (tamb√©m limpa).
        - Cria uma lista (final_order) contendo apenas as categorias predefinidas que realmente existem nos dados, mantendo a ordem. Avisa sobre categorias extras ou faltantes.

- OrdinalEncoder:
    - Instancia OrdinalEncoder passando a lista de categorias v√°lidas e ordenadas (ordinal_categories_final). Isso garante que a codifica√ß√£o num√©rica (0, 1, 2, ...) respeite a ordem correta.
        - handle_unknown='use_encoded_value', unknown_value=-1: Garante que valores n√£o previstos na ordem (incluindo 'Desconhecido' ou extras) recebam um valor num√©rico espec√≠fico (-1), em vez de causar erro.
        - Aplica (.fit_transform()) o encoder √†s colunas ordinais identificadas.

### Vari√°veis Nominais:
- Identifica√ß√£o: 
- Seleciona as colunas que foram inicialmente marcadas como categ√≥ricas mas n√£o foram processadas pelo OrdinalEncoder.
    - pd.get_dummies: Aplica o One-Hot Encoding apenas a essas colunas nominais. Isso cria novas colunas bin√°rias (0/1) para cada categoria √∫nica (ex: area_de_formacao_computacao, genero_feminino, setor_financas_ou_bancos).
        - dummy_na=False: N√£o cria coluna extra para NaNs (j√° tratados como 'Desconhecido').
        - drop_first=False: Mant√©m todas as categorias dummy, o que geralmente √© seguro para modelos de √°rvore.

### Resultado: 
- Todas as features agora s√£o num√©ricas (originais num√©ricas, ordinais codificadas, dummies bin√°rias). A lista features_final √© atualizada com os nomes de todas essas colunas. O n√∫mero final de features (106 neste caso) reflete essa transforma√ß√£o.

---

    # --- Etapa 4: Prepara√ß√£o para o Modelo ---
    
    # 4.1 Definir X e y
    X = df_model[features_final]
    y = df_model['salarymidpoint']
    
    # 4.2 Dividir em treino e teste
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
    
    # 4.3 Escalonamento das Features
    print("\nEscalonando features...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Converter de volta para DataFrame para manter nomes das colunas
    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
## üìà Explica√ß√£o (Split & Scale):

### Definir X e y: Separa o DataFrame df_model em:
- X: DataFrame contendo apenas as colunas de features (preditoras).
- y: Series contendo apenas a vari√°vel alvo num√©rica (salarymidpoint).

### Divis√£o Treino/Teste (train_test_split):
- Divide X e y em conjuntos de treinamento (X_train, y_train) e teste (X_test, y_test).
- test_size=0.25: Define que 25% dos dados ser√£o usados para teste, e 75% para treino.
- random_state=42: Garante que a divis√£o seja sempre a mesma se o c√≥digo for executado novamente (reprodutibilidade). Fundamental para comparar modelos.

### Escalonamento (StandardScaler):
- Objetivo: Transforma os dados para terem m√©dia 0 e desvio padr√£o 1. Embora modelos de √°rvore como LightGBM n√£o exijam escalonamento, ele geralmente n√£o prejudica e pode √†s vezes ajudar na converg√™ncia ou interpreta√ß√£o (especialmente com SHAP).
- Aplica√ß√£o:
    - Cria uma inst√¢ncia do StandardScaler.
    - scaler.fit_transform(X_train): Aprende a m√©dia e o desvio padr√£o apenas do conjunto de treino e aplica a transforma√ß√£o a ele.
    - scaler.transform(X_test): Aplica a mesma transforma√ß√£o (aprendida no treino) ao conjunto de teste. √â crucial n√£o usar fit no teste para evitar data leakage.

### Reconvers√£o para DataFrame: 
- Os dados escalados s√£o convertidos de volta para DataFrames (X_train_scaled_df, X_test_scaled_df) para preservar os nomes das colunas, que s√£o importantes para a interpreta√ß√£o posterior com SHAP.

---

    # --- Etapa 5: Treinamento e Otimiza√ß√£o do Modelo LightGBM ---
    print("\n--- Treinamento e Otimiza√ß√£o com GridSearchCV ---")
    
    lgbm = lgb.LGBMRegressor(objective='regression_l1', metric='mae', random_state=42, n_jobs=-1)
    
    param_grid = { # Define combina√ß√µes de hiperpar√¢metros a testar
        'n_estimators': [200, 400],
        'learning_rate': [0.05, 0.1],
        'num_leaves': [31, 50],
        'max_depth': [-1, 10],
        'min_child_samples': [20, 30],
    }
    
    grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid,
                               cv=3, scoring='r2', verbose=2, n_jobs=-1)
    
    print("Iniciando GridSearchCV...")
    grid_search.fit(X_train_scaled_df, y_train) # Treinar com dados escalados
    print("GridSearchCV conclu√≠do.")
## üöÄ Explica√ß√£o (Train - Otimiza√ß√£o):

### Instanciar Modelo Base: 
- Cria uma inst√¢ncia do lgb.LGBMRegressor com configura√ß√µes iniciais:
- objective='regression_l1': Define o objetivo como regress√£o, usando o Erro Absoluto M√©dio (L1 loss) como fun√ß√£o de perda a ser minimizada durante o treino.
- metric='mae': Usa MAE tamb√©m como m√©trica de avalia√ß√£o interna durante o treino (embora o GridSearchCV use R¬≤).
- random_state=42: Para reprodutibilidade interna do algoritmo.
- n_jobs=-1: Usa todos os processadores dispon√≠veis para acelerar.

### Definir Grid de Par√¢metros (param_grid): 
- Cria um dicion√°rio onde as chaves s√£o nomes de hiperpar√¢metros do LightGBM e os valores s√£o listas de op√ß√µes a serem testadas. Isso define o espa√ßo de busca para a otimiza√ß√£o.
- n_estimators: N√∫mero de √°rvores no ensemble.
- learning_rate: Taxa de aprendizado (qu√£o r√°pido o modelo aprende/corrige erros).
- num_leaves, max_depth, min_child_samples: Par√¢metros que controlam a complexidade de cada √°rvore individual para evitar overfitting.

### Configurar GridSearchCV:

- estimator=lgbm: O modelo base a ser otimizado.
- param_grid=param_grid: O grid de par√¢metros a testar.
- cv=3: Usa valida√ß√£o cruzada de 3 folds (divide o treino em 3 partes, treina em 2 e valida em 1, rotacionando). Isso d√° uma estimativa mais robusta do desempenho de cada combina√ß√£o de par√¢metros.
- scoring='r2': Usa o R¬≤ como m√©trica para decidir qual combina√ß√£o de par√¢metros √© a "melhor".
- verbose=2: Mostra logs detalhados durante a execu√ß√£o.
- n_jobs=-1: Paraleliza a busca nos folds/combina√ß√µes.

### Executar a Busca (grid_search.fit): 
- Treina o LightGBM com cada combina√ß√£o de par√¢metros do grid, usando valida√ß√£o cruzada (CV) no conjunto de treinamento escalado (X_train_scaled_df, y_train). Esta etapa pode ser demorada.

---

    # --- Etapa 6: Avalia√ß√£o do Modelo Otimizado ---
    print("\n--- Avalia√ß√£o do Modelo Otimizado ---")
    
    print("Melhores par√¢metros:", grid_search.best_params_)
    best_lgbm = grid_search.best_estimator_ # Pega o modelo treinado com os melhores params
    y_pred = best_lgbm.predict(X_test_scaled_df) # Prediz no teste escalado
    
    # Calcular M√©tricas
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    # Imprimir M√©tricas
    print(f"\nDesempenho no Teste:")
    print(f"MAE: R$ {mae:,.2f}")
    print(f"RMSE: R$ {rmse:,.2f}")
    print(f"R¬≤: {r2:.4f} ({r2*100:.2f}%)")
## üéØ Explica√ß√£o (Evaluate):

### Melhor Modelo:
- grid_search.best_params_: Acessa e imprime o dicion√°rio com a combina√ß√£o de hiperpar√¢metros que resultou no melhor score R¬≤ durante a valida√ß√£o cruzada.
- grid_search.best_estimator_: Acessa o modelo LightGBM que j√° foi re-treinado automaticamente pelo GridSearchCV usando os melhores par√¢metros encontrados e todo o conjunto de treino.

### Predi√ß√£o no Teste: 
- Usa o melhor modelo (best_lgbm) para fazer previs√µes (.predict()) no conjunto de teste escalado (X_test_scaled_df), que o modelo nunca viu antes.

### C√°lculo das M√©tricas: 
- Compara as previs√µes (y_pred) com os valores reais do conjunto de teste (y_test) usando as fun√ß√µes de m√©trica do sklearn.metrics:
- mean_absolute_error: Calcula o MAE.
- mean_squared_error: Calcula o MSE.
- np.sqrt(mse): Calcula o RMSE a partir do MSE.
- r2_score: Calcula o R¬≤.

### Apresenta√ß√£o: 
- Imprime as m√©tricas calculadas de forma formatada, permitindo avaliar o desempenho final do modelo otimizado em dados n√£o vistos.

---

    # --- Etapa 7: Interpreta√ß√£o e Visualiza√ß√£o (COM PLOTS SHAP CORRIGIDOS) ---
    
    # 7.1 Import√¢ncia Geral das Features (Gain)
    try:
        # ... (c√≥digo para obter feature_importances_ do best_lgbm) ...
        importance_df = pd.DataFrame(...)
        importance_df = importance_df[importance_df['importance'] > 0] # Filtra
        importance_df = importance_df.sort_values(...).head(30) # Pega Top 30
        # ... (c√≥digo seaborn.barplot para plotar) ...
        plt.show()
    except Exception as e:
        print(f"Erro plot feature importance: {e}")

## üìå Explica√ß√£o (Interpret - Feature Importance):

### Objetivo: 
- Identificar quais features tiveram maior impacto geral nas previs√µes do modelo LightGBM treinado.

### Acesso √† Import√¢ncia: 
- best_lgbm.feature_importances_ retorna um array com a import√¢ncia de cada feature, geralmente calculada por "gain" (redu√ß√£o total de erro que a feature proporcionou nos splits) ou "split" (n√∫mero de vezes que a feature foi usada para dividir).

### Processamento: 
- O c√≥digo cria um DataFrame para associar os nomes das features √†s suas import√¢ncias, filtra aquelas com import√¢ncia zero, ordena da mais para a menos importante e seleciona as Top 30.

### Visualiza√ß√£o: 
- Usa seaborn.barplot para criar um gr√°fico de barras horizontais mostrando as features mais importantes e seu respectivo valor de "Gain".

---

    # 7.2 Visualiza√ß√£o de √Årvore Individual
    try:
        graph = lgb.create_tree_digraph(best_lgbm, tree_index=0, ...) # Cria objeto graphviz
        graph.render(filename='...', format='png', view=False) # Salva imagem
        # ... (c√≥digo para exibir imagem no Colab) ...
    except Exception as e:
        print(f"Erro visualiza√ß√£o √°rvore: {e}.")
## üå≥ Explica√ß√£o (Interpret - Tree Visualization):

### Objetivo: 
- Entender a l√≥gica de decis√£o de uma das √°rvores individuais que comp√µem o modelo LightGBM (que √© um ensemble).

- lgb.create_tree_digraph: Fun√ß√£o do LightGBM que usa a biblioteca graphviz para gerar uma representa√ß√£o visual da estrutura de uma √°rvore espec√≠fica (aqui, tree_index=0, a primeira √°rvore). Mostra os n√≥s de decis√£o (condi√ß√µes), os ramos (verdadeiro/falso) e as folhas (previs√µes).

### Renderiza√ß√£o:
- .render() salva a visualiza√ß√£o como um arquivo de imagem (PNG).

### Exibi√ß√£o: 
- IPython.display.Image √© usado para tentar mostrar a imagem diretamente na sa√≠da do notebook Colab.

---

    # --- 7.3 Interpreta√ß√£o SHAP (COM CORRE√á√ÉO NOS NOMES) ---
    try:
        explainer = shap.TreeExplainer(best_lgbm) # Cria explicador para modelos de √°rvore
        shap_values = explainer.shap_values(X_test) # Calcula SHAP values no teste N√ÉO escalado
    
        # Plotar Resumo SHAP (dot)
        shap.summary_plot(shap_values, X_test, plot_type="dot", ...)
        plt.show()
    
        # Plotar Resumo SHAP (bar)
        shap.summary_plot(shap_values, X_test, plot_type="bar", ...)
        plt.show()
    
        # Plotar SHAP Dependence Plots (Usando nomes limpos CORRETOS)
        # ... (fun√ß√£o auxiliar get_ordinal_ticks_and_labels) ...
    
        # Exemplo 1: Experi√™ncia vs Sal√°rio (colorido por N√≠vel de Ensino)
        shap.dependence_plot(exp_col_clean, shap_values, X_test, interaction_index=edu_col_clean, ...)
        # ... (c√≥digo para adicionar ticks leg√≠veis) ...
        plt.show()
    
        # Exemplo 2: N√≠vel (Senioridade) vs Sal√°rio (colorido por Experi√™ncia)
        shap.dependence_plot(nivel_col_clean, shap_values, X_test, interaction_index=exp_col_clean, ...)
        # ... (c√≥digo para adicionar ticks leg√≠veis) ...
        plt.show()
    
        # Exemplo 3: Uso de Python vs Sal√°rio (colorido por Experi√™ncia)
        shap.dependence_plot(python_col_clean, shap_values, X_test, interaction_index=exp_col_clean, ...)
        # ... (c√≥digo para adicionar ticks leg√≠veis) ...
        plt.show()
    
    except Exception as e:
        print(f"\nErro inesperado SHAP: {e}")
        # ... (traceback para debug) ...
    
    print("\n--- Fim da An√°lise ---")
## üìä Explica√ß√£o (Interpret - SHAP):

### Objetivo: 
- Entender o impacto de cada feature nas previs√µes do modelo de forma mais detalhada e individualizada, incluindo intera√ß√µes. SHAP (SHapley Additive exPlanations) √© uma t√©cnica baseada em teoria dos jogos para explicar sa√≠das de modelos complexos.
- shap.TreeExplainer: Cria um objeto "explicador" otimizado para modelos baseados em √°rvore como o LightGBM.
- explainer.shap_values(X_test): Calcula os valores SHAP para cada feature e cada inst√¢ncia no conjunto de teste (X_test - importante usar os dados N√ÉO escalados aqui para que os plots mostrem os valores originais das features). O resultado √© geralmente um array onde cada linha corresponde a uma inst√¢ncia e cada coluna a uma feature.
- shap.summary_plot (dot): Cria o "beeswarm plot". Cada ponto √© uma inst√¢ncia/feature.
    - Eixo X: Valor SHAP (impacto na previs√£o).
    - Posi√ß√£o Vertical: Separa as features.
    - Cor: Valor original da feature (alto/baixo). Mostra se valores altos da feature aumentam (pontos vermelhos √† direita) ou diminuem (pontos azuis √† direita) a previs√£o.
    
- shap.summary_plot (bar): Mostra a import√¢ncia m√©dia absoluta de cada feature (m√©dia dos valores SHAP absolutos), fornecendo uma vis√£o geral similar ao Feature Importance do LightGBM, mas considerada mais robusta.
- shap.dependence_plot: Gr√°fico crucial para intera√ß√µes.
    - Eixo X: Valor de uma feature principal.
    - Eixo Y: Valor SHAP dessa feature principal (seu impacto na previs√£o).
    - Cor dos Pontos: Valor de uma segunda feature (de intera√ß√£o).
    ### Interpreta√ß√£o:
    - Mostra como o impacto da feature principal (Y) muda com seu pr√≥prio valor (X) e como essa rela√ß√£o √© influenciada pela feature de intera√ß√£o (cor).
    ###    Corre√ß√£o Aplicada:
    - O c√≥digo usa os nomes limpos e corretos das colunas (exp_col_clean, edu_col_clean, etc.) que existem em X_test. Inclui uma fun√ß√£o auxiliar (get_ordinal_ticks_and_labels) para tentar colocar os nomes originais das categorias (ex: 'J√∫nior', 'Pleno') nos eixos dos plots ordinais, tornando-os mais leg√≠veis.

### Tratamento de Erros: 
- O bloco try-except captura poss√≠veis erros durante o c√°lculo ou plotagem SHAP.

### print("\n--- Fim da An√°lise ---"): 
- Simplesmente indica que o script chegou ao final.
