{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11705758,"sourceType":"datasetVersion","datasetId":7347485}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, balanced_accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom collections import Counter\nimport os\nimport warnings\nimport joblib\nfrom sklearn.inspection import permutation_importance\nfrom scipy.stats import chi2_contingency\nimport matplotlib.cm as cm\nimport scipy.sparse\nimport time\n\n# Configurações para visualização\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\nsns.set_palette('viridis')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\n# Função para tratamento de erros\ndef safe_execution(func, error_message=\"Ocorreu um erro\", *args, **kwargs):\n    try:\n        return func(*args, **kwargs)\n    except Exception as e:\n        print(f\"{error_message}: {e}\")\n        return None\n\n# Função para calcular o coeficiente de Cramer's V\ndef cramers_v(x, y):\n    \"\"\"Calcula o coeficiente de Cramer's V entre duas variáveis categóricas.\"\"\"\n    confusion_matrix = pd.crosstab(x, y)\n    chi2 = chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2 / n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n    rcorr = r - ((r-1)**2)/(n-1)\n    kcorr = k - ((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n\n# Função para detectar outliers usando IQR\ndef detect_outliers(df, column):\n    \"\"\"Detecta outliers usando o método IQR.\"\"\"\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n    return outliers\n\n# Função para agrupar faixas salariais\ndef group_salary_ranges(df, salary_column):\n    \"\"\"Agrupa faixas salariais próximas para reduzir o número de classes.\"\"\"\n    salary_mapping = {\n        'Menos de R$ 1.000/mês': 'Até R$ 2.000/mês',\n        'de R$ 1.001/mês a R$ 2.000/mês': 'Até R$ 2.000/mês',\n        'de R$ 101/mês a R$ 2.000/mês': 'Até R$ 2.000/mês',\n        'de R$ 2.001/mês a R$ 3.000/mês': 'R$ 2.001/mês a R$ 4.000/mês',\n        'de R$ 3.001/mês a R$ 4.000/mês': 'R$ 2.001/mês a R$ 4.000/mês',\n        'de R$ 4.001/mês a R$ 6.000/mês': 'R$ 4.001/mês a R$ 8.000/mês',\n        'de R$ 6.001/mês a R$ 8.000/mês': 'R$ 4.001/mês a R$ 8.000/mês',\n        'de R$ 8.001/mês a R$ 12.000/mês': 'R$ 8.001/mês a R$ 16.000/mês',\n        'de R$ 12.001/mês a R$ 16.000/mês': 'R$ 8.001/mês a R$ 16.000/mês',\n        'de R$ 16.001/mês a R$ 20.000/mês': 'R$ 16.001/mês a R$ 30.000/mês',\n        'de R$ 20.001/mês a R$ 25.000/mês': 'R$ 16.001/mês a R$ 30.000/mês',\n        'de R$ 25.001/mês a R$ 30.000/mês': 'R$ 16.001/mês a R$ 30.000/mês',\n        'de R$ 30.001/mês a R$ 40.000/mês': 'Acima de R$ 30.000/mês',\n        'Acima de R$ 40.001/mês': 'Acima de R$ 30.000/mês'\n    }\n    \n    df['Faixa salarial agrupada'] = df[salary_column].map(salary_mapping)\n    \n    print(\"\\nDistribuição das faixas salariais agrupadas:\")\n    print(df['Faixa salarial agrupada'].value_counts())\n    \n    return df\n\n# Carregamento dos dados\ntry:\n    file_path = '/kaggle/input/dataset-clean/dados_limpos.csv'\n    if not os.path.exists(file_path):\n        file_path = 'dados_limpos.csv'\n    df = pd.read_csv(file_path)\n    print(f\"Dados carregados com sucesso de: {file_path}\")\nexcept Exception as e:\n    print(f\"Erro ao carregar os dados: {e}\")\n    raise Exception(\"Não foi possível carregar o arquivo de dados. Verifique o caminho.\")\n\n# Exibindo informações básicas sobre os dados\nprint(\"\\nInformações sobre os dados:\")\nprint(f\"Número de registros: {df.shape[0]}\")\nprint(f\"Número de colunas: {df.shape[1]}\")\nprint(\"\\nColunas disponíveis:\")\nprint(df.columns.tolist())\n\n# Análise exploratória básica\nprint(\"\\nDistribuição da faixa salarial original:\")\nsalary_dist = df['Faixa salarial mensal'].value_counts()\nprint(salary_dist)\n\n# Visualizando a distribuição salarial original\nplt.figure(figsize=(14, 8))\nsns.countplot(y=df['Faixa salarial mensal'], order=df['Faixa salarial mensal'].value_counts().index)\nplt.title('Distribuição de Faixas Salariais Originais', fontsize=16)\nplt.xlabel('Contagem', fontsize=14)\nplt.ylabel('Faixa Salarial', fontsize=14)\nplt.tight_layout()\nplt.savefig('distribuicao_faixas_salariais_originais.png')\nplt.close()\n\n# Agrupando faixas salariais para reduzir o número de classes\ndf = group_salary_ranges(df, 'Faixa salarial mensal')\n\n# Visualizando a distribuição salarial agrupada\nplt.figure(figsize=(14, 8))\nsns.countplot(y=df['Faixa salarial agrupada'], order=df['Faixa salarial agrupada'].value_counts().index)\nplt.title('Distribuição de Faixas Salariais Agrupadas', fontsize=16)\nplt.xlabel('Contagem', fontsize=14)\nplt.ylabel('Faixa Salarial', fontsize=14)\nplt.tight_layout()\nplt.savefig('distribuicao_faixas_salariais_agrupadas.png')\nplt.close()\n\n# Análise de correlação entre variáveis categóricas e o target\ntry:\n    categorical_cols = ['Nível de ensino alcançado', 'Área de formação acadêmica', \n                       'Tempo de experiência na área de dados', 'Nível de senioridade',\n                       'Gênero do profissional', 'Cor/Raça/Etnia', 'Setor de atuação da empresa',\n                       'UF onde mora', 'Cargo atual']\n    \n    corr_with_target = {}\n    for col in categorical_cols:\n        if col in df.columns:\n            corr_with_target[col] = cramers_v(df[col], df['Faixa salarial agrupada'])\n    \n    corr_with_target = {k: v for k, v in sorted(corr_with_target.items(), key=lambda item: item[1], reverse=True)}\n    print(\"\\nCorrelação (Cramer's V) entre variáveis categóricas e faixa salarial agrupada:\")\n    for col, corr in corr_with_target.items():\n        print(f\"{col}: {corr:.4f}\")\n    \n    plt.figure(figsize=(12, 8))\n    plt.bar(corr_with_target.keys(), corr_with_target.values())\n    plt.xticks(rotation=45, ha='right')\n    plt.title(\"Correlação (Cramer's V) com Faixa Salarial Agrupada\", fontsize=16)\n    plt.ylabel(\"Coeficiente de Cramer's V\", fontsize=14)\n    plt.tight_layout()\n    plt.savefig('correlacao_variaveis_faixa_salarial.png')\n    plt.close()\nexcept Exception as e:\n    print(f\"Erro ao calcular correlações: {e}\")\n\n# Preparação dos dados para o modelo\nfeatures = df[categorical_cols]\ntarget = df['Faixa salarial agrupada']\n\n# Codificando o target\nle_target = LabelEncoder()\ny = le_target.fit_transform(target)\n\n# Mapeamento dos códigos para as classes\ntarget_mapping = dict(zip(range(len(le_target.classes_)), le_target.classes_))\nprint(\"\\nMapeamento das classes codificadas:\")\nfor code, label in target_mapping.items():\n    print(f\"{code}: {label}\")\n\n# Divisão estratificada em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(\n    features, y, test_size=0.3, random_state=42, stratify=y\n)\nprint(f\"\\nDados divididos em: {X_train.shape[0]} amostras de treino e {X_test.shape[0]} amostras de teste\")\n\n# Pré-processamento: transformação de variáveis categóricas\ncategorical_features = features.columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n    ])\n\n# Aplicar preprocessador ao conjunto de treino\nX_train_transformed = preprocessor.fit_transform(X_train)\n\n# Balanceamento de classes usando oversampling manual (mais rápido que SMOTE)\ntry:\n    from sklearn.utils import resample\n    \n    if scipy.sparse.issparse(X_train_transformed):\n        X_train_dense = X_train_transformed.toarray()\n    else:\n        X_train_dense = X_train_transformed\n    \n    X_resampled_list = []\n    y_resampled_list = []\n    \n    class_counts = pd.Series(y_train).value_counts()\n    majority_size = class_counts.max()\n    \n    for class_idx in np.unique(y_train):\n        class_indices = np.where(y_train == class_idx)[0]\n        class_features = X_train_dense[class_indices]\n        class_targets = np.array([class_idx] * len(class_indices))\n        \n        if len(class_indices) < majority_size:\n            n_samples = majority_size\n            resampled_features, resampled_targets = resample(\n                class_features, class_targets, \n                replace=True, \n                n_samples=n_samples, \n                random_state=42\n            )\n        else:\n            resampled_features, resampled_targets = class_features, class_targets\n        \n        X_resampled_list.append(resampled_features)\n        y_resampled_list.append(resampled_targets)\n    \n    X_train_resampled = np.vstack(X_resampled_list)\n    y_train_resampled = np.concatenate(y_resampled_list)\n    \n    print(\"Distribuição das classes após o balanceamento:\")\n    resampled_class_dist = pd.Series(y_train_resampled).value_counts().sort_index()\n    for class_idx, count in resampled_class_dist.items():\n        print(f\"{target_mapping[class_idx]}: {count} ({count/len(y_train_resampled)*100:.2f}%)\")\n        \nexcept Exception as e:\n    print(f\"Erro ao realizar balanceamento: {e}\")\n    if scipy.sparse.issparse(X_train_transformed):\n        X_train_resampled = X_train_transformed.toarray()\n    else:\n        X_train_resampled = X_train_transformed\n    y_train_resampled = y_train\n\n# **CORREÇÃO PRINCIPAL: Grid de parâmetros otimizado e mais eficiente**\ngb_clf = GradientBoostingClassifier(random_state=42)\n\n# Grid reduzido para ser mais eficiente (de 486 para 36 combinações)\nparam_grid_reduced = {\n    'n_estimators': [100, 200],  # Reduzido de 3 para 2 opções\n    'learning_rate': [0.1, 0.05],  # Reduzido de 3 para 2 opções\n    'max_depth': [3, 5, 7],  # Mantido 3 opções\n    'min_samples_split': [2, 10],  # Reduzido de 3 para 2 opções\n    'min_samples_leaf': [1, 4],  # Reduzido de 3 para 2 opções\n    'subsample': [0.8]  # Reduzido para 1 opção\n}\n\n# Alternativa ainda mais rápida: RandomizedSearchCV\nparam_dist = {\n    'n_estimators': [50, 100, 150, 200],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'max_depth': [3, 4, 5, 6, 7],\n    'min_samples_split': [2, 5, 10, 15],\n    'min_samples_leaf': [1, 2, 4, 6],\n    'subsample': [0.8, 0.9, 1.0]\n}\n\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Reduzido de 5 para 3 folds\n\nprint(\"\\nEscolha o método de otimização:\")\nprint(\"1. GridSearchCV com grid reduzido (36 combinações, ~5-10 min)\")\nprint(\"2. RandomizedSearchCV (20 combinações aleatórias, ~3-5 min)\")\nprint(\"3. Modelo com parâmetros padrão otimizados (instantâneo)\")\n\n# Para automatizar, vamos usar RandomizedSearchCV (opção 2)\noptimization_choice = 2\n\nif optimization_choice == 1:\n    print(\"\\nIniciando GridSearchCV com grid reduzido...\")\n    start_time = time.time()\n    grid_search = GridSearchCV(\n        gb_clf, param_grid_reduced, cv=cv, \n        scoring='balanced_accuracy', n_jobs=-1, verbose=1\n    )\n    grid_search.fit(X_train_resampled, y_train_resampled)\n    best_gb = grid_search.best_estimator_\n    print(f\"Tempo de execução: {time.time() - start_time:.2f} segundos\")\n    print(\"Melhores parâmetros:\", grid_search.best_params_)\n    \nelif optimization_choice == 2:\n    print(\"\\nIniciando RandomizedSearchCV...\")\n    start_time = time.time()\n    random_search = RandomizedSearchCV(\n        gb_clf, param_dist, n_iter=20, cv=cv,\n        scoring='balanced_accuracy', n_jobs=-1, verbose=1, random_state=42\n    )\n    random_search.fit(X_train_resampled, y_train_resampled)\n    best_gb = random_search.best_estimator_\n    print(f\"Tempo de execução: {time.time() - start_time:.2f} segundos\")\n    print(\"Melhores parâmetros:\", random_search.best_params_)\n    \nelse:\n    print(\"\\nUsando parâmetros otimizados pré-definidos...\")\n    best_gb = GradientBoostingClassifier(\n        n_estimators=150,\n        learning_rate=0.1,\n        max_depth=5,\n        min_samples_split=10,\n        min_samples_leaf=4,\n        subsample=0.8,\n        random_state=42\n    )\n    best_gb.fit(X_train_resampled, y_train_resampled)\n\n# Transformar X_test\nX_test_transformed = preprocessor.transform(X_test)\nif scipy.sparse.issparse(X_test_transformed):\n    X_test_transformed = X_test_transformed.toarray()\n\n# Previsões\ny_pred = best_gb.predict(X_test_transformed)\n\n# Avaliação do modelo\nprint(\"\\nAcurácia no conjunto de teste: {:.4f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"Acurácia balanceada no conjunto de teste: {:.4f}\".format(balanced_accuracy_score(y_test, y_pred)))\n\n# Relatório de classificação\nprint(\"\\nRelatório de Classificação:\")\nprint(classification_report(y_test, y_pred, \n                          target_names=[target_mapping[i] for i in sorted(np.unique(y_test))], \n                          zero_division=0))\n\n# Matriz de confusão\nplt.figure(figsize=(16, 14))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n           xticklabels=[target_mapping[i] for i in sorted(np.unique(y_test))], \n           yticklabels=[target_mapping[i] for i in sorted(np.unique(y_test))])\nplt.title('Matriz de Confusão', fontsize=16)\nplt.xlabel('Predito', fontsize=14)\nplt.ylabel('Real', fontsize=14)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig('matriz_confusao.png')\nplt.close()\nprint(\"Matriz de confusão salva como 'matriz_confusao.png'\")\n\n# Salvando o modelo treinado\ntry:\n    model_filename = 'modelo_gradient_boosting_disparidade_salarial_otimizado.pkl'\n    joblib.dump(best_gb, model_filename)\n    print(f\"\\nModelo salvo como '{model_filename}'\")\n    \n    preprocessor_filename = 'preprocessador_otimizado.pkl'\n    joblib.dump(preprocessor, preprocessor_filename)\n    print(f\"Preprocessador salvo como '{preprocessor_filename}'\")\n    \n    target_mapping_filename = 'target_mapping.pkl'\n    joblib.dump(target_mapping, target_mapping_filename)\n    print(f\"Mapeamento do target salvo como '{target_mapping_filename}'\")\nexcept Exception as e:\n    print(f\"Erro ao salvar o modelo: {e}\")\n\nprint(\"\\nModelo treinado e avaliado com sucesso!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:13:46.469841Z","iopub.execute_input":"2025-05-23T10:13:46.470156Z","iopub.status.idle":"2025-05-23T10:16:43.195591Z","shell.execute_reply.started":"2025-05-23T10:13:46.470131Z","shell.execute_reply":"2025-05-23T10:16:43.194621Z"}},"outputs":[{"name":"stdout","text":"Dados carregados com sucesso de: /kaggle/input/dataset-clean/dados_limpos.csv\n\nInformações sobre os dados:\nNúmero de registros: 3300\nNúmero de colunas: 13\n\nColunas disponíveis:\n['id', 'Nível de ensino alcançado', 'Área de formação acadêmica', 'Faixa salarial mensal', 'Tempo de experiência na área de dados', 'Nível de senioridade', 'Gênero do profissional', 'Cor/Raça/Etnia', 'Setor de atuação da empresa', 'UF onde mora', 'Cargo atual', 'Oportunidade de aprendizado', 'Reputação da empresa']\n\nDistribuição da faixa salarial original:\nFaixa salarial mensal\nde R$ 8.001/mês a R$ 12.000/mês     790\nde R$ 4.001/mês a R$ 6.000/mês      635\nde R$ 6.001/mês a R$ 8.000/mês      538\nde R$ 12.001/mês a R$ 16.000/mês    388\nde R$ 3.001/mês a R$ 4.000/mês      303\nde R$ 2.001/mês a R$ 3.000/mês      229\nde R$ 1.001/mês a R$ 2.000/mês      172\nde R$ 16.001/mês a R$ 20.000/mês    113\nde R$ 20.001/mês a R$ 25.000/mês     53\nde R$ 25.001/mês a R$ 30.000/mês     30\nde R$ 30.001/mês a R$ 40.000/mês     20\nMenos de R$ 1.000/mês                15\nAcima de R$ 40.001/mês               13\nde R$ 101/mês a R$ 2.000/mês          1\nName: count, dtype: int64\n\nDistribuição das faixas salariais agrupadas:\nFaixa salarial agrupada\nR$ 8.001/mês a R$ 16.000/mês     1178\nR$ 4.001/mês a R$ 8.000/mês      1173\nR$ 2.001/mês a R$ 4.000/mês       532\nR$ 16.001/mês a R$ 30.000/mês     196\nAté R$ 2.000/mês                  188\nAcima de R$ 30.000/mês             33\nName: count, dtype: int64\n\nCorrelação (Cramer's V) entre variáveis categóricas e faixa salarial agrupada:\nNível de senioridade: 0.5506\nTempo de experiência na área de dados: 0.2984\nNível de ensino alcançado: 0.2277\nCargo atual: 0.1862\nSetor de atuação da empresa: 0.0938\nUF onde mora: 0.0787\nÁrea de formação acadêmica: 0.0698\nGênero do profissional: 0.0437\nCor/Raça/Etnia: 0.0428\n\nMapeamento das classes codificadas:\n0: Acima de R$ 30.000/mês\n1: Até R$ 2.000/mês\n2: R$ 16.001/mês a R$ 30.000/mês\n3: R$ 2.001/mês a R$ 4.000/mês\n4: R$ 4.001/mês a R$ 8.000/mês\n5: R$ 8.001/mês a R$ 16.000/mês\n\nDados divididos em: 2310 amostras de treino e 990 amostras de teste\nDistribuição das classes após o balanceamento:\nAcima de R$ 30.000/mês: 825 (16.67%)\nAté R$ 2.000/mês: 825 (16.67%)\nR$ 16.001/mês a R$ 30.000/mês: 825 (16.67%)\nR$ 2.001/mês a R$ 4.000/mês: 825 (16.67%)\nR$ 4.001/mês a R$ 8.000/mês: 825 (16.67%)\nR$ 8.001/mês a R$ 16.000/mês: 825 (16.67%)\n\nEscolha o método de otimização:\n1. GridSearchCV com grid reduzido (36 combinações, ~5-10 min)\n2. RandomizedSearchCV (20 combinações aleatórias, ~3-5 min)\n3. Modelo com parâmetros padrão otimizados (instantâneo)\n\nIniciando RandomizedSearchCV...\nFitting 3 folds for each of 20 candidates, totalling 60 fits\nTempo de execução: 171.95 segundos\nMelhores parâmetros: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 6, 'learning_rate': 0.2}\n\nAcurácia no conjunto de teste: 0.5273\nAcurácia balanceada no conjunto de teste: 0.4015\n\nRelatório de Classificação:\n                               precision    recall  f1-score   support\n\n       Acima de R$ 30.000/mês       0.12      0.10      0.11        10\n             Até R$ 2.000/mês       0.28      0.41      0.33        56\nR$ 16.001/mês a R$ 30.000/mês       0.40      0.31      0.35        59\n  R$ 2.001/mês a R$ 4.000/mês       0.42      0.42      0.42       160\n  R$ 4.001/mês a R$ 8.000/mês       0.57      0.45      0.50       352\n R$ 8.001/mês a R$ 16.000/mês       0.61      0.72      0.66       353\n\n                     accuracy                           0.53       990\n                    macro avg       0.40      0.40      0.40       990\n                 weighted avg       0.53      0.53      0.52       990\n\nMatriz de confusão salva como 'matriz_confusao.png'\n\nModelo salvo como 'modelo_gradient_boosting_disparidade_salarial_otimizado.pkl'\nPreprocessador salvo como 'preprocessador_otimizado.pkl'\nMapeamento do target salvo como 'target_mapping.pkl'\n\nModelo treinado e avaliado com sucesso!\n","output_type":"stream"}],"execution_count":1}]}